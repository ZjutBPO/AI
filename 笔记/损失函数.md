# 交叉熵损失函数「Cross Entropy Loss」
参考资料：https://www.jianshu.com/p/b07f4cd32ba6
$$L=-[ylog\widehat{y}+(1-y)log(1-\widehat{y})]$$